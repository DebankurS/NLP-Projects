{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deban\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train.csv\",sep=\"~\",usecols=['Description','Is_Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sentence_to_words(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    sentence = ''.join([c for c in sentence if c not in punctuation])    \n",
    "    sentence_split = sentence.split('\\n')\n",
    "    sentence = ' '.join(sentence_split)\n",
    "    words = sentence.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def cleanup_data(data,cache_file=\"preprocessed_data.pkl\"):\n",
    "    cache_data=None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(cache_file,'rb') as f:\n",
    "                cache_data=pickle.load(f)\n",
    "            print(\"Read cache data\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    #Create cache if not present\n",
    "    if cache_data is None:\n",
    "        words_train=[sentence_to_words(sentence) for sentence in data]\n",
    "        #Write to cache file\n",
    "        if cache_file is not None:\n",
    "            cache_data=words_train\n",
    "            with open(cache_file, \"wb\") as f:\n",
    "                pickle.dump(cache_data,f)\n",
    "            print(\"Wrote preprocessed data to: \",cache_file)\n",
    "    else:\n",
    "        words_train=cache_data\n",
    "    return words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read cache data\n"
     ]
    }
   ],
   "source": [
    "data.Description=cleanup_data(data.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    flattened_data = [y for x in data for y in x]\n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    word_count=Counter(flattened_data)\n",
    "    sorted_words = [word for word, _ in word_count.most_common()]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size-2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict=build_dict(data.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=200):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=200):\n",
    "    result = []\n",
    "      \n",
    "    for sentence in data:\n",
    "        converted= convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Description=convert_and_pad_data(word_dict,data.Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "data.Is_Response=le.fit_transform(data.Is_Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.Description, data.Is_Response, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 256\n",
    "\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (20215, 80)\n",
      "x_test shape: (9957, 80)\n",
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\Anaconda3\\envs\\tfdeeplearn\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\Anaconda3\\envs\\tfdeeplearn\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 20215 samples, validate on 9957 samples\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\Anaconda3\\envs\\tfdeeplearn\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "20215/20215 [==============================] - 11s 563us/sample - loss: 0.6256 - acc: 0.6841 - val_loss: 0.6257 - val_acc: 0.6766\n",
      "Epoch 2/30\n",
      "20215/20215 [==============================] - 10s 503us/sample - loss: 0.6197 - acc: 0.6902 - val_loss: 0.6256 - val_acc: 0.6795\n",
      "Epoch 3/30\n",
      "20215/20215 [==============================] - 10s 505us/sample - loss: 0.6158 - acc: 0.7002 - val_loss: 0.6254 - val_acc: 0.6800\n",
      "Epoch 4/30\n",
      "20215/20215 [==============================] - 10s 506us/sample - loss: 0.6097 - acc: 0.7024 - val_loss: 0.6245 - val_acc: 0.6818\n",
      "Epoch 5/30\n",
      "20215/20215 [==============================] - 10s 507us/sample - loss: 0.6056 - acc: 0.7002 - val_loss: 0.6394 - val_acc: 0.6771\n",
      "Epoch 6/30\n",
      "20215/20215 [==============================] - 10s 505us/sample - loss: 0.6033 - acc: 0.7036 - val_loss: 0.6345 - val_acc: 0.6837\n",
      "Epoch 7/30\n",
      "20215/20215 [==============================] - 10s 507us/sample - loss: 0.5961 - acc: 0.7070 - val_loss: 0.6368 - val_acc: 0.6808\n",
      "Epoch 8/30\n",
      "20215/20215 [==============================] - 10s 508us/sample - loss: 0.5895 - acc: 0.7117 - val_loss: 0.6306 - val_acc: 0.6929\n",
      "Epoch 9/30\n",
      "20215/20215 [==============================] - 11s 533us/sample - loss: 0.5840 - acc: 0.7177 - val_loss: 0.6285 - val_acc: 0.6938\n",
      "Epoch 10/30\n",
      "20215/20215 [==============================] - 11s 524us/sample - loss: 0.5809 - acc: 0.7210 - val_loss: 0.6326 - val_acc: 0.6919\n",
      "Epoch 11/30\n",
      "20215/20215 [==============================] - 11s 535us/sample - loss: 0.5815 - acc: 0.7194 - val_loss: 0.6379 - val_acc: 0.6907\n",
      "Epoch 12/30\n",
      "20215/20215 [==============================] - 11s 521us/sample - loss: 0.5777 - acc: 0.7232 - val_loss: 0.6453 - val_acc: 0.6828\n",
      "Epoch 13/30\n",
      "20215/20215 [==============================] - 10s 517us/sample - loss: 0.5807 - acc: 0.7199 - val_loss: 0.6345 - val_acc: 0.6833\n",
      "Epoch 14/30\n",
      "20215/20215 [==============================] - 10s 518us/sample - loss: 0.5881 - acc: 0.7163 - val_loss: 0.6350 - val_acc: 0.6915\n",
      "Epoch 15/30\n",
      "20215/20215 [==============================] - 10s 519us/sample - loss: 0.5820 - acc: 0.7192 - val_loss: 0.6292 - val_acc: 0.6921\n",
      "Epoch 16/30\n",
      "20215/20215 [==============================] - 10s 515us/sample - loss: 0.5775 - acc: 0.7221 - val_loss: 0.6339 - val_acc: 0.6921\n",
      "Epoch 17/30\n",
      "20215/20215 [==============================] - 10s 517us/sample - loss: 0.5766 - acc: 0.7214 - val_loss: 0.6424 - val_acc: 0.6909\n",
      "Epoch 18/30\n",
      "20215/20215 [==============================] - 10s 508us/sample - loss: 0.5759 - acc: 0.7199 - val_loss: 0.6370 - val_acc: 0.6921\n",
      "Epoch 19/30\n",
      "20215/20215 [==============================] - 10s 513us/sample - loss: 0.5917 - acc: 0.7100 - val_loss: 0.6342 - val_acc: 0.6797\n",
      "Epoch 20/30\n",
      "20215/20215 [==============================] - 10s 517us/sample - loss: 0.5939 - acc: 0.7089 - val_loss: 0.6365 - val_acc: 0.6886\n",
      "Epoch 21/30\n",
      "20215/20215 [==============================] - 11s 522us/sample - loss: 0.5817 - acc: 0.7196 - val_loss: 0.6448 - val_acc: 0.6909\n",
      "Epoch 22/30\n",
      "20215/20215 [==============================] - 10s 515us/sample - loss: 0.5781 - acc: 0.7212 - val_loss: 0.6352 - val_acc: 0.6904\n",
      "Epoch 23/30\n",
      "20215/20215 [==============================] - 11s 526us/sample - loss: 0.5772 - acc: 0.7216 - val_loss: 0.6348 - val_acc: 0.6910\n",
      "Epoch 24/30\n",
      "20215/20215 [==============================] - 10s 518us/sample - loss: 0.5752 - acc: 0.7219 - val_loss: 0.6435 - val_acc: 0.6912\n",
      "Epoch 25/30\n",
      "20215/20215 [==============================] - 11s 522us/sample - loss: 0.5726 - acc: 0.7242 - val_loss: 0.6458 - val_acc: 0.6909\n",
      "Epoch 26/30\n",
      "20215/20215 [==============================] - 10s 515us/sample - loss: 0.5714 - acc: 0.7238 - val_loss: 0.6450 - val_acc: 0.6906\n",
      "Epoch 27/30\n",
      "20215/20215 [==============================] - 10s 519us/sample - loss: 0.5701 - acc: 0.7245 - val_loss: 0.6458 - val_acc: 0.6904\n",
      "Epoch 28/30\n",
      "20215/20215 [==============================] - 10s 518us/sample - loss: 0.5674 - acc: 0.7267 - val_loss: 0.6475 - val_acc: 0.6896\n",
      "Epoch 29/30\n",
      " 1792/20215 [=>............................] - ETA: 8s - loss: 0.5601 - acc: 0.7349"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=30,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
